{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64b487b1",
   "metadata": {},
   "source": [
    "# Streaming Pipeline\n",
    "This example is to show how streaming pipeline can be done in Hopsworks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69befa65",
   "metadata": {},
   "source": [
    "## 1. Login\n",
    "\n",
    "We will first start by initiating the Spark application and logging in to Hopsworks.\n",
    "\n",
    "Do note that Hopsworks's streaming capabilities are only working with Spark/PySpark server, aka. you must run your code in Spark server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2f226df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>1</td><td>application_1687425332507_0002</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"/hopsworks-api/yarnui/https://ip-172-31-3-253.ec2.internal:8089/proxy/application_1687425332507_0002/\">Link</a></td><td><a target=\"_blank\" href=\"/hopsworks-api/yarnui/https://ip-172-31-3-253.ec2.internal:8044/node/containerlogs/container_1687425332507_0002_01_000001/moneylion__meb10000\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "\n",
      "Logged in to project, explore it here https://44.199.168.211/p/119\n",
      "Connected. Call `.close()` to terminate connection gracefully."
     ]
    }
   ],
   "source": [
    "import hopsworks\n",
    "\n",
    "project = hopsworks.login()\n",
    "\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7d58a0d9",
   "metadata": {},
   "source": [
    "## 2. Kafka Connection\n",
    "Hopsworks can connect to your Kafka broker via **storage connector**.\n",
    "\n",
    "Currently there is no way of creating storage connector via coding, all storage connectors must be created via UI and retrieve using `get_storage_connector` function.\n",
    "\n",
    "![storage connector](https://github.com/cjbeh/Hopsworks-POC/blob/master/resources/storage_connector.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7695cec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "connector = fs.get_storage_connector(\"moneylion_kafka\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fca68aa",
   "metadata": {},
   "source": [
    "## 3. Transform / Aggregate\n",
    "a. Read the Kafka event using `read_stream` function from Kafka connector. Specify which topic you would like to consume the data.\n",
    "\n",
    "b. Parse the Kafka event data to the expected structure.\n",
    "\n",
    "c. Define your transformation / aggregation using PySpark.\n",
    "\n",
    "ps. `read_stream` is returning a **streaming Spark dataframe** instead of conventional Spark dataframe. Conventional dataframe is more or less similar to `pandas` dataframe while streaming dataframe has more complexity compared to the conventional one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "79716686",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, window, col, sum, udf, when\n",
    "from pyspark.sql.types import StructField, StructType, StringType, DoubleType, TimestampType, LongType, IntegerType, BooleanType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "06b7ba58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- total_amount: long (nullable = true)\n",
      " |-- essential_spending: long (nullable = true)\n",
      " |-- created_at: timestamp (nullable = true)"
     ]
    }
   ],
   "source": [
    "# Read data stream from Kafka\n",
    "df = connector.read_stream(topic='user-transaction-full')\n",
    "\n",
    "full_schema = StructType([StructField('user_id', StringType(), True),\n",
    "                          StructField('amount', IntegerType(), True),\n",
    "                          StructField('description', StringType(), True),\n",
    "                          StructField('created_at', StringType(), True)])\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def categorize(description):\n",
    "    if \"restaurant\" in description.lower():\n",
    "        return \"F&B\"\n",
    "    if \"bill\" in description.lower():\n",
    "        return \"COMMITMENT\"\n",
    "    return \"OTHERS\"\n",
    "\n",
    "# Deserialise data from and create streaming query\n",
    "df = df.selectExpr(\"CAST(value AS STRING)\")\\\n",
    "                   .select(from_json(\"value\", full_schema).alias(\"value\"))\\\n",
    "                   .select(\"value.user_id\", \"value.amount\", \"value.description\", \"value.created_at\")\\\n",
    "                   .selectExpr(\"CAST(user_id as string)\", \"CAST(amount as int)\", \"CAST(description as string)\", \"CAST(created_at as timestamp)\")\\\n",
    "                   .withColumn(\"category\", categorize(col(\"description\")))\\\n",
    "                   .groupBy(\"user_id\", window(\"created_at\", \"2 days\", \"1 days\"))\\\n",
    "                   .agg(sum(\"amount\").alias(\"total_amount\"), sum(when(col(\"category\") != \"OTHERS\", col(\"amount\")).otherwise(0)).alias(\"essential_spending\"))\\\n",
    "                   .select(\"user_id\", \"total_amount\", \"essential_spending\", \"window.end\")\\\n",
    "                   .withColumnRenamed(\"end\", \"created_at\")\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41cf214",
   "metadata": {},
   "source": [
    "You can always confirm the final schema of your streaming dataframe using `printSchema` function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70ab2bcd",
   "metadata": {},
   "source": [
    "## 4. Feature Group\n",
    "**Feature group** in Hopsworks is something similar to batch / streaming feature view in Tecton. In Hopsworks, both batch and stream are using the same feature group, we do not differentiate between them.\n",
    "\n",
    "Creating a feature group is just as simple as calling `get_or_create_feature_group` function.\n",
    "\n",
    "a. set `online_enabled` to `True` if you want the data of this feature group to be saved to online store.\n",
    "\n",
    "b. set `stream` to `True` to use Hopsworks streaming write API (this is the preferred setup). This is related to how data is saved to online store, it has nothing related to the type (batch/stream) of this feature.\n",
    "\n",
    "After created the feature group, use `insert_stream` function to ingest the transformed / aggregated streaming dataframe we have from step 3. After calling the function, it will create a streaming Spark job to continuously ingest, transform and save the feature to online store until `stop` is called or an error is hit.\n",
    "\n",
    "When you ingest the streaming dataframe into the feature group, Hopsworks will figure out the suitable data type for every features in both offline and online store for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "707cdca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Group created successfully, explore it at \n",
      "https://44.199.168.211/p/119/fs/67/fg/15\n",
      "<pyspark.sql.streaming.StreamingQuery object at 0x7faa8b9a3910>\n",
      "StatisticsWarning: Stream ingestion for feature group `user_transaction_complex`, with version `1` will not compute statistics."
     ]
    }
   ],
   "source": [
    "user_transaction_complex = fs.get_or_create_feature_group(\n",
    "    name=\"user_transaction_complex\",\n",
    "    version=1,\n",
    "    description=\"User transaction complex amount\",\n",
    "    primary_key=['user_id'],\n",
    "    event_time='created_at',\n",
    "    online_enabled=True,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "user_transaction_complex.insert_stream(df, output_mode=\"update\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1026f925",
   "metadata": {},
   "source": [
    "Feature group created\n",
    "![feature group 1](https://github.com/cjbeh/Hopsworks-POC/blob/master/resources/feature_group_1.png)\n",
    "\n",
    "Data in online store\n",
    "![online store](https://github.com/cjbeh/Hopsworks-POC/blob/master/resources/online_store.png)\n",
    "\n",
    "Data in offline store\n",
    "![offline store](https://github.com/cjbeh/Hopsworks-POC/blob/master/resources/offline_store.png)\n",
    "\n",
    "Feature statistics computed\n",
    "![feature statistics](https://github.com/cjbeh/Hopsworks-POC/blob/master/resources/feature_statistics.png)\n",
    "\n",
    "There are other things like great expectation (not supported for streaming dataframe), alerts, feature correlation, monitoring metrics and etc."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4391e74d",
   "metadata": {},
   "source": [
    "## 5. Feature chaining\n",
    "Hopsworks do not really support feature stacking as well. If feature group B is consuming data from feature group A which is getting data from a Kafka topic, feature group B will not get updated whenever feature group A is updated.\n",
    "\n",
    "However, you can always define the chain of features in the same streaming pipeline.\n",
    "\n",
    "a. Consume data from Kafka.\n",
    "\n",
    "b. Transform as feature group A and save it to online store.\n",
    "\n",
    "c. Continue using the result streaming dataframe from (b.) to do transformation/aggregation for feature group B and save it to online store.\n",
    "\n",
    "In this way, whenever there is new data coming in, both feature group A and B will get the updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "087e58bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- is_big_spent: boolean (nullable = true)\n",
      " |-- created_at: timestamp (nullable = true)"
     ]
    }
   ],
   "source": [
    "@udf(returnType=BooleanType())\n",
    "def is_big_spent(non_essential_spending):\n",
    "    if non_essential_spending > 90:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "df = df.withColumn(\"non_essential_spending\", col(\"total_amount\") - col(\"essential_spending\"))\\\n",
    "       .withColumn(\"is_big_spent\", is_big_spent(col(\"non_essential_spending\")))\\\n",
    "       .select(\"user_id\", \"is_big_spent\", \"created_at\")\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c845d106",
   "metadata": {},
   "source": [
    "Here we save the new transformation result as another feature group named `user_transaction_big_spent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ceb023bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Group created successfully, explore it at \n",
      "https://44.199.168.211/p/119/fs/67/fg/16\n",
      "<pyspark.sql.streaming.StreamingQuery object at 0x7faa8b93e4c0>\n",
      "StatisticsWarning: Stream ingestion for feature group `user_transaction_big_spent`, with version `1` will not compute statistics."
     ]
    }
   ],
   "source": [
    "user_transaction_big_spent = fs.get_or_create_feature_group(\n",
    "    name=\"user_transaction_big_spent\",\n",
    "    version=1,\n",
    "    description=\"User transaction complex amount\",\n",
    "    primary_key=['user_id'],\n",
    "    event_time='created_at',\n",
    "    online_enabled=True,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "user_transaction_big_spent.insert_stream(df, output_mode=\"update\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d27f1dd7",
   "metadata": {},
   "source": [
    "![feature group 2](https://github.com/cjbeh/Hopsworks-POC/blob/master/resources/feature_group_2.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "705d449a",
   "metadata": {},
   "source": [
    "To stop the streaming Spark query created, call `stop` function with the Job ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "751126ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.streams.get(spark.streams.active[0].id).stop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e5bce90f",
   "metadata": {},
   "source": [
    "To stop the Spark cluster, call `spark.stop()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b068f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
